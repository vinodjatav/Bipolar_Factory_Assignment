{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bipolar_Factory_Assignment(Crawl News).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TkgZVzpiXYo",
        "colab_type": "text"
      },
      "source": [
        "# **Importing Required Packages!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH4jP4hYdVKO",
        "colab_type": "code",
        "outputId": "c67bb2b6-3269-49c6-e0c6-2eb674157081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# The Python itertools module is a collection of tools for handling iterators. Simply put, iterators are data types that can be used in a for loop. The most common iterator in Python is the list.\n",
        "import itertools\n",
        "# The Natural Language Toolkit (NLTK) is a Python package for natural language processing.\n",
        "import nltk\n",
        "# Requests is a Python module that you can use to send all kinds of HTTP requests. It is an easy-to-use library with a lot of features ranging from passing parameters in URLs to sending custom headers and SSL Verification.\n",
        "import requests\n",
        "# Gensim is designed to handle large text collections using data streaming and incremental online algorithms.\n",
        "import gensim\n",
        "# bs4 â€” BeautifulSoup 4. Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem.porter import *\n",
        "# Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "040YJwndjHez",
        "colab_type": "text"
      },
      "source": [
        "# **Assign News Websites URL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4_YkBi2jG4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract news from below websites\n",
        "urls=[\n",
        "      ['https://news.google.com','a','DY5T1d'],\n",
        "    ['https://www.rediff.com/news','h2','hdtitle'],\n",
        "    ['https://www.scoopwhoop.com/','a','article-title'],\n",
        "   ['https://www.yahoo.com/news/','h3','Mb(5px)'],\n",
        "   ['https://www.pinkvilla.com/','div','ypromoted'],\n",
        "   ['https://www.buzzfeednews.com/','h2','newsblock-story-card__title']\n",
        "  \n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOBoC51RaGIC",
        "colab_type": "text"
      },
      "source": [
        "# **First Step:** Extract the Headline from Viral News Websites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_qiJRlWk6T0",
        "colab_type": "code",
        "outputId": "41c754e8-df29-4836-d8ba-623531f0060b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "source": [
        "no_of_titles=0\n",
        "titles=[]\n",
        "title_url=[]\n",
        "print(\"\\n\\n\\n   Extracting headlines of \",len(urls) ,\" news websites\")\n",
        "for url in urls:\n",
        "    r1 = requests.get(url[0])\n",
        "    coverpage = r1.content\n",
        "    soup1 = BeautifulSoup(coverpage, 'html.parser')\n",
        "    coverpage_news = soup1.find_all(url[1],class_=url[2])\n",
        "    no_headlines_per_website=0\n",
        "    for heading in coverpage_news:\n",
        "        headline=heading.get_text()\n",
        "        headline=headline.strip()\n",
        "        titles.append(headline)\n",
        "        title_url.append([url[0],headline])\n",
        "        no_of_titles += 1\n",
        "        no_headlines_per_website+=1\n",
        "    print(\"\\n\\t\\tWebsite : \" , url[0], '\\n\\t\\tNo. of headlines collected : ', no_headlines_per_website)\n",
        "\n",
        "print(\"\\n   Total No. of headlines : \",no_of_titles)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "   Extracting headlines of  6  news websites\n",
            "\n",
            "\t\tWebsite :  https://news.google.com \n",
            "\t\tNo. of headlines collected :  196\n",
            "\n",
            "\t\tWebsite :  https://www.rediff.com/news \n",
            "\t\tNo. of headlines collected :  25\n",
            "\n",
            "\t\tWebsite :  https://www.scoopwhoop.com/ \n",
            "\t\tNo. of headlines collected :  17\n",
            "\n",
            "\t\tWebsite :  https://www.yahoo.com/news/ \n",
            "\t\tNo. of headlines collected :  30\n",
            "\n",
            "\t\tWebsite :  https://www.pinkvilla.com/ \n",
            "\t\tNo. of headlines collected :  17\n",
            "\n",
            "\t\tWebsite :  https://www.buzzfeednews.com/ \n",
            "\t\tNo. of headlines collected :  35\n",
            "\n",
            "   Total No. of headlines :  320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mzWHgWbVTfR",
        "colab_type": "text"
      },
      "source": [
        "# **Second Step:** Pre processs the text and convert to tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV5_Yu0zVSxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))       \n",
        "    return result\n",
        "\n",
        "tokanized_titles=[preprocess(title) for title in titles]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0hAfiucVpOp",
        "colab_type": "text"
      },
      "source": [
        "# **Third Step:** Build the Bow and LDA models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu1h3EmjV0Ci",
        "colab_type": "code",
        "outputId": "7e94c48f-687e-414c-9054-16b21950eaf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(tokanized_titles)\n",
        "corpus = [dictionary.doc2bow(list_of_tokens) for list_of_tokens in tokanized_titles]\n",
        "num_topics = 15\n",
        "lda_model = gensim.models.LdaModel(corpus,\n",
        "                                    num_topics=num_topics, \n",
        "                                    id2word=dictionary,\n",
        "                                    passes=4, \n",
        "                                    alpha=[0.01]*num_topics,\n",
        "                                    eta=[0.01]*len(dictionary.keys()))\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log\n",
            "  diff = np.log(self.expElogbeta)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvGVDFSMV9MG",
        "colab_type": "text"
      },
      "source": [
        "# **Visualised The Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2gTPp1VWUVt",
        "colab_type": "code",
        "outputId": "3a9d4d80-8c00-4b8f-ea9e-eb0652b769e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        }
      },
      "source": [
        "print('\\n\\n   Results :')\n",
        "topic_words=[]\n",
        "for topic,words in lda_model.show_topics(num_topics=num_topics, num_words=4, log=False, formatted=False):\n",
        "    word_list=[]\n",
        "    for word,prob in words:\n",
        "        word_list.append(word)\n",
        "    topic_words.append(word_list)\n",
        "\n",
        "index_list=[]\n",
        "for topic in topic_words:\n",
        "    for i in range(len(tokanized_titles)):\n",
        "        if(set(topic).issubset(set(tokanized_titles[i]))):\n",
        "            index_list.append(i)\n",
        "\n",
        "trending=[]\n",
        "for i in set(index_list):\n",
        "    trending.append(title_url[i])\n",
        "trends=list(k for k,_ in itertools.groupby(trending))\n",
        "\n",
        "for aritcle in trends:\n",
        "    print('\\n\\t Headline : ',aritcle[1],'\\n\\t Website : ',aritcle[0])\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "   Results :\n",
            "\n",
            "\t Headline :  Brazil's star justice minister SÃ©rgio Moro resigns in blow to Jair Bolsonaro \n",
            "\t Website :  https://news.google.com\n",
            "\n",
            "\t Headline :  Who Plays Magda on 'Penny Dreadful: City of Angels'? Natalie Dormer Portrays Showtime's Latest Supernatural Antagonist \n",
            "\t Website :  https://news.google.com\n",
            "\n",
            "\t Headline :  In â€˜Penny Dreadful: City of Angels,â€™ California Dreaming, Darkly \n",
            "\t Website :  https://news.google.com\n",
            "\n",
            "\t Headline :  Penny Dreadful: City of Angels Review: Often Very Silly, Rarely Any Fun \n",
            "\t Website :  https://news.google.com\n",
            "\n",
            "\t Headline :  'Penny Dreadful: City of Angels' Review \n",
            "\t Website :  https://news.google.com\n",
            "\n",
            "\t Headline :  Natalie Dormer on Why She Wanted to Be Filmed from the â€˜Worstâ€™ Angle in â€˜Penny Dreadful: City of Angelsâ€™ \n",
            "\t Website :  https://news.google.com\n",
            "\n",
            "\t Headline :  Giant asteroid flying by Earth next week looks like it's wearing a face mask \n",
            "\t Website :  https://news.google.com\n",
            "\n",
            "\t Headline :  Massive asteroid flying past Earth next week looks like it's wearing a face mask \n",
            "\t Website :  https://news.google.com\n",
            "\n",
            "\t Headline :  A massive asteroid is approaching Earth next week. It looks like it's wearing a face mask. \n",
            "\t Website :  https://news.google.com\n",
            "\n",
            "\t Headline :  Brazilâ€™s justice minister resigns, accusing Bolsonaro of improper conduct \n",
            "\t Website :  https://news.google.com\n",
            "\n",
            "\t Headline :  Brazil justice minister quits in dispute with Bolsonaro \n",
            "\t Website :  https://news.google.com\n",
            "\n",
            "\t Headline :  Turmoil in Brazil: Bolsonaro Fires Police Chief and Justice Minister Quits \n",
            "\t Website :  https://news.google.com\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}